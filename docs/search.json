[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "Homework 5 - ST 558",
    "section": "",
    "text": "A random forest model could use cross-validation to determine what the number of predictors we want to randomly select from the set of predictors for each model fitting in a bootstrap sample.\n\n\n\n\nThe bagged tree algorithm is a way of averaging of values of several regression/classification trees to determine the optimal splitting values. A bootstrap sample is needed by treating the sample as the population and selecting several (100-1000) samples from it, running (training) the model on each of the samples and obtaining the distribution of our statistic. For regression trees, prediction can be done by taking the average of the distribution, and for Classification trees, prediction can be done by a choice of the majority of the classification for a new data value. A bagged tree algorithm specifically uses all of the predictors in the dataset.\n\n\n\n\nA General Linear Model refers to a predictive model with a response variable that is continuous, rather than one that is binary or a count. Both continuous and categorical variables can be used as predictors for the response.\n\n\n\n\nAdding an interaction term to a model changes the interpretation of the coefficients, specifically it keeps the model from only looking at the unique/main effect of a predictor variable. Tells us if the effect of one predictor variable on the response is different for different values of the other predictor.\n\n\n\n\nWe split our data into testing and training data sets when conducting predictive modeling because we want to be able to generalize our model. In other words, we do not want to overfit the model and have it not be reliable in prediction of new data. They can also be used as a means of comparison to check that the final model is working correctly."
  },
  {
    "objectID": "Homework5.html#task-1-conceptual-questions",
    "href": "Homework5.html#task-1-conceptual-questions",
    "title": "Homework 5 - ST 558",
    "section": "",
    "text": "A random forest model could use cross-validation to determine what the number of predictors we want to randomly select from the set of predictors for each model fitting in a bootstrap sample.\n\n\n\n\nThe bagged tree algorithm is a way of averaging of values of several regression/classification trees to determine the optimal splitting values. A bootstrap sample is needed by treating the sample as the population and selecting several (100-1000) samples from it, running (training) the model on each of the samples and obtaining the distribution of our statistic. For regression trees, prediction can be done by taking the average of the distribution, and for Classification trees, prediction can be done by a choice of the majority of the classification for a new data value. A bagged tree algorithm specifically uses all of the predictors in the dataset.\n\n\n\n\nA General Linear Model refers to a predictive model with a response variable that is continuous, rather than one that is binary or a count. Both continuous and categorical variables can be used as predictors for the response.\n\n\n\n\nAdding an interaction term to a model changes the interpretation of the coefficients, specifically it keeps the model from only looking at the unique/main effect of a predictor variable. Tells us if the effect of one predictor variable on the response is different for different values of the other predictor.\n\n\n\n\nWe split our data into testing and training data sets when conducting predictive modeling because we want to be able to generalize our model. In other words, we do not want to overfit the model and have it not be reliable in prediction of new data. They can also be used as a means of comparison to check that the final model is working correctly."
  },
  {
    "objectID": "Homework5.html#quick-edadata-preparation",
    "href": "Homework5.html#quick-edadata-preparation",
    "title": "Homework 5 - ST 558",
    "section": "Quick EDA/Data Preparation",
    "text": "Quick EDA/Data Preparation\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(gbm)\n\n\nCheck missingness of the variables and review the data\n\n\nheart_data &lt;- read.csv(\"heart.csv\", header = T)\n# Assess if there are missing values for the data (0 represents missing in Age, RestingBP, Cholesterol, and MaxHR)\napply(heart_data, MARGIN = 2, function(x) sum(is.na(x)))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n# Check for values of 0 in the specified variables and convert those to NA\nheart_data[,c(1,4,5,8)][heart_data[,c(1,4,5,8)] == 0] &lt;- NA\n# Calculate number of missing values now\napply(heart_data, MARGIN = 2, function(x) sum(is.na(x)))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              1            172 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\nstr(heart_data)\n\n'data.frame':   918 obs. of  12 variables:\n $ Age           : int  40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr  \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr  \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : int  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : int  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr  \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : int  172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr  \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope      : chr  \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease  : int  0 1 0 1 0 0 0 0 1 0 ...\n\n# Summary table for categorical variables\ntable(heart_data$HeartDisease)\n\n\n  0   1 \n410 508 \n\nsummary(heart_data)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   : 80.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.5  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n                                                       NA's   :1      \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   : 85.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:207.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :237.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :244.6   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:275.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n NA's   :172                                                        \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n                                                                         \n\n# Males found to have a much higher relative proportion of heart disease in this sample\ntable(heart_data$HeartDisease, heart_data$Sex)\n\n   \n      F   M\n  0 143 267\n  1  50 458\n\n# Those with exercise angina have much higher relative proportion of heart disease in this sample\ntable(heart_data$HeartDisease, heart_data$ExerciseAngina)\n\n   \n      N   Y\n  0 355  55\n  1 192 316\n\n# Those with the \"ASY\" chest pain type have a higher relative proportion of heart disease\ntable(heart_data$HeartDisease, heart_data$ChestPainType)\n\n   \n    ASY ATA NAP  TA\n  0 104 149 131  26\n  1 392  24  72  20\n\n\n\nCreate a new variable that is a factor of the HeartDisease variable, remove the ST_Slope variable and original HeartDisease variable.\n\n\n# Create factor version of the HeartDisease variable\nheart_data$HeartDisease &lt;- factor(heart_data$HeartDisease)\n# Drop the ST_Slope variable and filter out the missing variables\nclean_heart_data &lt;- na.omit(heart_data[,-11])\n\nstr(clean_heart_data)\n\n'data.frame':   746 obs. of  11 variables:\n $ Age           : int  40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr  \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr  \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : int  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : int  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr  \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : int  172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr  \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease  : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:172] 294 295 296 297 298 299 300 301 302 303 ...\n  ..- attr(*, \"names\")= chr [1:172] \"294\" \"295\" \"296\" \"297\" ...\n\n\n\nUse dummyVars() and predict() to create new columns to be added to the data frame, preparing for the kNN model to be fit\n\n\ndummy_inc &lt;- dummyVars( ~ ., data = clean_heart_data)\ndummy_columns &lt;- as.data.frame(predict(dummy_inc, newdata = clean_heart_data))\nfinal_data &lt;- dplyr::select(dummy_columns, -HeartDisease.0)\nfinal_data$HeartDisease.1 &lt;- factor(final_data$HeartDisease.1)"
  },
  {
    "objectID": "Homework5.html#split-the-data",
    "href": "Homework5.html#split-the-data",
    "title": "Homework 5 - ST 558",
    "section": "Split the Data",
    "text": "Split the Data\n\nSplit the data into the training and test sets\n\n\n# Set seed to be able to replicate results\nset.seed(10)\ntraining_index &lt;- createDataPartition(final_data$HeartDisease.1, p = 0.7, list = F)\ntraining &lt;- as.data.frame(final_data[training_index,])\ntesting &lt;- as.data.frame(final_data[-training_index,])"
  },
  {
    "objectID": "Homework5.html#knn",
    "href": "Homework5.html#knn",
    "title": "Homework 5 - ST 558",
    "section": "kNN",
    "text": "kNN\n\nUse 10 fold cross-validation, repeated 3 times, to train the model on the training dataset\n\n\nknn_fit &lt;- train(HeartDisease.1 ~ ., data = training,\n             method = \"knn\",\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n             tuneGrid = data.frame(k = c(1:40)))\n\n# getModelInfo(knn_fit)\nknn_fit\n\nk-Nearest Neighbors \n\n523 samples\n 17 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 470, 471, 470, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7501572  0.4984992\n   2  0.7317610  0.4619323\n   3  0.7916909  0.5820584\n   4  0.7929124  0.5844327\n   5  0.8069182  0.6126679\n   6  0.8011611  0.6009583\n   7  0.8113933  0.6214417\n   8  0.8114538  0.6216639\n   9  0.8158563  0.6307237\n  10  0.8100750  0.6190114\n  11  0.7999153  0.5986199\n  12  0.8056483  0.6099988\n  13  0.8055999  0.6097581\n  14  0.8030600  0.6046966\n  15  0.8055999  0.6095891\n  16  0.8068457  0.6121309\n  17  0.8049589  0.6082140\n  18  0.8081398  0.6148529\n  19  0.8062046  0.6108091\n  20  0.8080914  0.6147323\n  21  0.8036647  0.6057137\n  22  0.8049468  0.6084716\n  23  0.8018021  0.6019898\n  24  0.8011248  0.6008007\n  25  0.7999274  0.5982097\n  26  0.8024915  0.6033216\n  27  0.8056241  0.6097028\n  28  0.8042937  0.6071316\n  29  0.8062893  0.6111898\n  30  0.8056483  0.6099097\n  31  0.8082245  0.6152207\n  32  0.8127117  0.6242182\n  33  0.8095186  0.6177877\n  34  0.8075472  0.6136181\n  35  0.8024552  0.6035759\n  36  0.8063014  0.6114958\n  37  0.8043541  0.6075679\n  38  0.8056483  0.6102022\n  39  0.8056483  0.6102135\n  40  0.8088050  0.6164666\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 9.\n\n\n\nPredict classification of HeartDisease for the observations in the testing dataset and compare them to the actual values of the HeartDisease variable.\n\n\n# Generate Confusion matrix to assess the accuracy of the model in predicting Heart Diseases after running our model on the testing dataset\ntest_preds &lt;- predict(knn_fit, newdata = testing)\nconfusionMatrix(test_preds, testing$HeartDisease.1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 95 25\n         1 22 81\n                                          \n               Accuracy : 0.7892          \n                 95% CI : (0.7298, 0.8408)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.267e-16       \n                                          \n                  Kappa : 0.5769          \n                                          \n Mcnemar's Test P-Value : 0.7705          \n                                          \n            Sensitivity : 0.8120          \n            Specificity : 0.7642          \n         Pos Pred Value : 0.7917          \n         Neg Pred Value : 0.7864          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4260          \n   Detection Prevalence : 0.5381          \n      Balanced Accuracy : 0.7881          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#logistic-regression",
    "href": "Homework5.html#logistic-regression",
    "title": "Homework 5 - ST 558",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nRun 3 different logistic regression models for predicting HeartDisease classification. Fit the models on the training dataset.\n\n\n# Generate the 3 logistic regression models \n# Logistic model including the main effect of ExerciseAngina, RestingBP, and Cholesterol\nlogist_mod1 &lt;- train(HeartDisease.1 ~ ExerciseAnginaY + RestingBP + Cholesterol, \n                   data = training,\n                   method = \"glm\",\n                   preProcess = c(\"center\", \"scale\"),\n                   trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3))\n\n# Logistic model for \nlogist_mod2 &lt;- train(HeartDisease.1 ~ ExerciseAnginaY + SexM + RestingBP + Cholesterol + Age, \n                   data = training,\n                   method = \"glm\",\n                   preProcess = c(\"center\", \"scale\"),\n                   trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3))\n\n# Logistic model using all predictors\nlogist_mod3 &lt;- train(HeartDisease.1 ~ ., \n                   data = training,\n                   method = \"glm\",\n                   preProcess = c(\"center\", \"scale\"),\n                   trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3))\n\n\nIdentify the best model and provide a basic summary for it.\n\n\nmod1_preds &lt;- predict(logist_mod1, newdata = testing)\npostResample(mod1_preds, testing$HeartDisease.1)\n\n Accuracy     Kappa \n0.8026906 0.6019473 \n\nmod2_preds &lt;- predict(logist_mod2, newdata = testing)\npostResample(mod2_preds, testing$HeartDisease.1)\n\n Accuracy     Kappa \n0.8206278 0.6387787 \n\nmod3_preds &lt;- predict(logist_mod3, newdata = testing)\npostResample(mod3_preds, testing$HeartDisease.1)\n\n Accuracy     Kappa \n0.7982063 0.5938069 \n\n\nThe best model is the second one, which uses the subjectâ€™s Sex and the indicator variable of if they have Angina while Exercising or not, along with the continuous predictors of Resting Blood Pressure, Cholesterol, and Age.\nThe confusion matrix for this model was found to be:\n\nconfusionMatrix(mod2_preds, testing$HeartDisease.1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 102  25\n         1  15  81\n                                          \n               Accuracy : 0.8206          \n                 95% CI : (0.7639, 0.8687)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6388          \n                                          \n Mcnemar's Test P-Value : 0.1547          \n                                          \n            Sensitivity : 0.8718          \n            Specificity : 0.7642          \n         Pos Pred Value : 0.8031          \n         Neg Pred Value : 0.8438          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4574          \n   Detection Prevalence : 0.5695          \n      Balanced Accuracy : 0.8180          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#tree-models",
    "href": "Homework5.html#tree-models",
    "title": "Homework 5 - ST 558",
    "section": "Tree Models",
    "text": "Tree Models\n\nCreate the classification tree model using the same predictors as the model from the previous section\n\n\ntree_fit &lt;- train(HeartDisease.1 ~ ExerciseAnginaY + SexM + RestingBP + Cholesterol + Age, \n                  data = training,\n                  method = \"rpart\",\n                  preProcess = c(\"center\", \"scale\"),\n                  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n                  tuneGrid = data.frame(cp = seq(from = 0, to = 0.1, by = 0.001)))\n\n# tree_fit\n\n\nCreate the random forest model using random subsets of our 5 predictors.\n\n\nrf_fit &lt;- train(HeartDisease.1 ~ ExerciseAnginaY + SexM + RestingBP + Cholesterol + Age, \n                data = training,\n                method = \"rf\",\n                preProcess = c(\"center\", \"scale\"),\n                trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n                tuneGrid = data.frame(mtry = 1:5))\n\n# rf_fit\n\n\nCreate the boosted tree model using cross validation for different combinations of our tuning parameters\n\n\nbagged_fit &lt;- train(HeartDisease.1 ~ ExerciseAnginaY + SexM + RestingBP + Cholesterol + Age,\n                    data = training,\n                    method = \"gbm\",\n                    preProcess = c(\"center\", \"scale\"),\n                    trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n                    tuneGrid = data.frame(expand.grid(n.trees = c(25, 50, 100, 200), interaction.depth = c(1,2,3), shrinkage = 0.1, n.minobsinnode = 10)),\n                    verbose = F)\n\n# bagged_fit\n\n\nCheck how well each of these models does on the test set by creating Confusion matrices.\n\n\ntree_preds &lt;- predict(tree_fit, newdata = testing)\nconfusionMatrix(tree_preds, testing$HeartDisease.1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 93 22\n         1 24 84\n                                          \n               Accuracy : 0.7937          \n                 95% CI : (0.7346, 0.8448)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.5868          \n                                          \n Mcnemar's Test P-Value : 0.8828          \n                                          \n            Sensitivity : 0.7949          \n            Specificity : 0.7925          \n         Pos Pred Value : 0.8087          \n         Neg Pred Value : 0.7778          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4170          \n   Detection Prevalence : 0.5157          \n      Balanced Accuracy : 0.7937          \n                                          \n       'Positive' Class : 0               \n                                          \n\nrf_preds &lt;- predict(rf_fit, newdata = testing)\nconfusionMatrix(rf_preds, testing$HeartDisease.1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 102  29\n         1  15  77\n                                          \n               Accuracy : 0.8027          \n                 95% CI : (0.7443, 0.8528)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.6019          \n                                          \n Mcnemar's Test P-Value : 0.05002         \n                                          \n            Sensitivity : 0.8718          \n            Specificity : 0.7264          \n         Pos Pred Value : 0.7786          \n         Neg Pred Value : 0.8370          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4574          \n   Detection Prevalence : 0.5874          \n      Balanced Accuracy : 0.7991          \n                                          \n       'Positive' Class : 0               \n                                          \n\nbagged_preds &lt;- predict(bagged_fit, newdata = testing)\nconfusionMatrix(bagged_preds, testing$HeartDisease.1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 95 25\n         1 22 81\n                                          \n               Accuracy : 0.7892          \n                 95% CI : (0.7298, 0.8408)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.267e-16       \n                                          \n                  Kappa : 0.5769          \n                                          \n Mcnemar's Test P-Value : 0.7705          \n                                          \n            Sensitivity : 0.8120          \n            Specificity : 0.7642          \n         Pos Pred Value : 0.7917          \n         Neg Pred Value : 0.7864          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4260          \n   Detection Prevalence : 0.5381          \n      Balanced Accuracy : 0.7881          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThe model that did the best job, in terms of accuracy, on the test set, was the Random Forest Model. The specific random forest that produced the highest accuracy, was the one that used a parameter value of mtry = 1."
  }
]